---
title: "Machine Learning course project"
author: "Alex Butylev"
date: '20 May 2017'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
  library(caret); library(randomForest)
  training <- read.csv(file = 'pml-training.csv', header = TRUE, colClasses = 'character', dec = '.')
  training[, 1] <- as.integer(training[, 1])
  training[, 2] <- as.factor(training[, 2])
  training[, 3] <- as.integer(training[, 3])
  training[, 4] <- as.integer(training[, 4])
  training[, 5] <- as.Date(training[, 5])
  training[, 6] <- as.factor(training[, 6])
  for (i in 7:159) {training[, i] <- as.numeric(training[, i])}
  training[, 160] <- as.factor(training[, 160])
  tra <- training[, 8:160]

```

## Summary

This document describes the model I built for Weight Lifting Exercise Dataset. The data for this project comes from this source: http://groupware.les.inf.puc-rio.br/har. The dataset collected a large number of variables, which values got from four accelerometers. The purpose of model is to relate variable **classe** to data from devices.

The **Random Forest** method (library randomForest) was used for model for reasons which will be mentioned further.
The model showed extremely good accuracy equal 1.000 for the training set and 0.996 for the testing set.

## Exploratory analisys

The **pml-training.csv** is wide dataset with target variable **classe** and 152 potential regressors.The data for the dataset came from four accelerometers on the belt, forearm, arm, and dumbell of 6 participants, who were asked to perform the same excercise in five different ways (response factor variable **classe** with five levels "A", "B", "C", "D" and "E"). The purpose of the model - to detect the manner in which the excercise was done, i.e. to predict response **classe**. 
There's a lot of NA values in data and I decided to get rid of all variables, which have any number of NA values.I have to mention that this action deleted some potentially useful information from data. If further model wouldn't show an excellent accuracy it would had made me to cancel this deletion. 

```{r data simplifying}
  var.delete <- sapply(tra, function(x) sum(is.na(x)) != 0)
  tra.new <- tra[ , !var.delete]
  dim(tra.new)
```

This operation left me with 53 variables, including **classe**. Then I simplified the task even more and decided not to take into consideration the fact that the dataset has a time structure and thus there are obvious restrictions for values of **classe** variable. In other words I considered every row as an independent event relatively adjacent rows though it's not true. Here I lost an another portion of information but I hoped that it wouldn't cost too much for model's accuracy.

I didn't perform any other explorations as they seemed too questionable for so wide dataset.

So I divided dataset for training set and testing set, leaving 80% of data in the training set and the rest in the testing set:
```{r data partition}
    set.seed(31417)
    inTrain <- createDataPartition(tra.new$classe, p = .8, list = F)
    train_f <- tra.new[inTrain, ]; test_f <- tra.new[-inTrain, ]
```

## Model selection

I didn't hesitate long for a model class and chose **Random forest** for these arguments:

1) qualitative outcome variable;

2) too much regressors (52);

3) no need for preprocessing (like data normalisation);

4) main argument - **Random Forest** doesn't need cross-validation.

The argument, which let me to omit cross-validation came from this Breiman's source (http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr): 
"In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows: 
Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. 
Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests."

I used library **randomForest** for model training:
```{r model}
    model <- randomForest(classe ~ ., train_f, ntree=500, importance=T)
```

First of all the plot of error was built:
```{r plot}
    plot(model)
```

The plot indicates that after 100 decision trees, there is not a significant reduction in error rate. So we could decrease value of parameter **ntree** to 100.
Also we can view variable Importance Plot and find out top 5 variables selected based on Model Accuracy and Gini value.
``` {r variance plot}
varImpPlot(model, sort = T, main="Variable Importance", n.var=5)
```

So here we can see that **yaw_belt** and **roll_belt** variables are the most important.

## Model accuracy and the conclusion

In the final step let's check model accuracy on training and testing sets.

``` {r train accuracy}
  train_f$predicted <- predict(model, train_f)
  tmp_1 <- confusionMatrix(data = train_f$predicted, reference=train_f$classe, positive='yes')
  test_f$predicted <- predict(model, test_f)
  tmp_2 <- confusionMatrix(data = test_f$predicted, reference=test_f$classe, positive='yes')
  tmp_1$table; tmp_1$overall
  tmp_2$table; tmp_2$overall 
```


We've got absolute model accuracy (1.000) for the training set and almost absolute (0.996) for the testing set. As we used random forest method and checked results on testing set we have all reasons to take this model as the final one.It's interesting, the dataset has a number of excessive variables, that let us to exclude them from model and made it much simpler.

``` {r test, include=FALSE}
  testing <- read.csv(file = 'pml-testing.csv', header = TRUE, colClasses = 'character', dec = '.')
  for (i in 7:159) {testing[, i] <- as.numeric(testing[, i])}
  testing[, 160] <- as.factor(testing[, 160])
  tes <- testing[, 8:160]
  var.delete <- sapply(tes, function(x) sum(is.na(x)) != 0)
  tes.new <- tes[ , !var.delete]
  tes.new$predicted <- predict(model, tes.new)
  knitr::opts_chunk$set(echo = F)
```

At the end here I applied model to the 20 test cases:
``` {r results}
  tes.new$predicted
```
